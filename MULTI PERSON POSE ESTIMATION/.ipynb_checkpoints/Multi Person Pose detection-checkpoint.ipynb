{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd9e4e0e-c660-4c1e-8574-74f8d6a44e70",
   "metadata": {},
   "source": [
    "# Install and import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b803be06-189c-405a-b637-ce6695b7f758",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79301a12-a04d-490a-914f-a240c8aa51e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f76ab350-62b8-483a-af22-7153782f8ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd6e4490-6d66-41da-9c0b-c7107ccc20b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb01a016-7ce1-447b-9c67-345460917f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4b6fce-d0f9-46a1-b731-0f467b831c6f",
   "metadata": {},
   "source": [
    "# Load Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d38d9311-2612-4edc-8817-0eb8a2c9fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '1.tflite'\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c17081-3a77-4449-aab4-3bc819d7c5f0",
   "metadata": {},
   "source": [
    "# Make Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4bcd9cee-70dc-414c-984a-5d8d2f5edefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# People running - aspect ratio = 0.5625 , resize chosen - 192 , 320\n",
    "# cap = cv.VideoCapture(\"people_running.mp4\")\n",
    "cap = cv.VideoCapture(\"football_2.mp4\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    #### Input and resizing\n",
    "    image = frame.copy()\n",
    "    image = tf.image.resize_with_pad(np.expand_dims(image,axis=0),192,320)\n",
    "    image = tf.cast(image,dtype=tf.uint8)\n",
    "\n",
    "    # ### set output and input\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    is_dynamic_shape_model = input_details[0]['shape_signature'][2] == -1 # if true, means it is dynamic model\n",
    "    if is_dynamic_shape_model:\n",
    "        input_tensor_index = input_details[0]['index']\n",
    "        input_shape = image.shape\n",
    "        interpreter.resize_tensor_input(\n",
    "            input_tensor_index, input_shape, strict=True)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # #make Predictions\n",
    "    interpreter.set_tensor(input_details[0]['index'], image.numpy())\n",
    "    interpreter.invoke()\n",
    "    results = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "    keypoints_with_scores = results[:,:,:51].reshape((6,17,3))\n",
    "    draw_keypoints_connections(frame,keypoints_with_scores,EDGES,0.4)\n",
    "    draw_keypoints(frame,keypoints_with_scores,0.4)\n",
    "\n",
    "    cv.imshow(\"Frame\",frame)\n",
    "   \n",
    "    if cv.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e8a543-6add-4d2a-a5ec-580fc62d7bf4",
   "metadata": {},
   "source": [
    "# Testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2165856b-3ba1-4214-8d3e-5ed68722cbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.squeeze(tf.image.resize_with_pad(np.expand_dims(frame,axis=0),256,256))\n",
    "plt.imshow(cv.cvtColor(img,cv.COLOR_BGR2RGB).astype(np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85fce474-6079-4b27-b2a3-7937357cea5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'serving_default_input:0',\n",
       "  'index': 0,\n",
       "  'shape': array([1, 1, 1, 3]),\n",
       "  'shape_signature': array([ 1, -1, -1,  3]),\n",
       "  'dtype': numpy.uint8,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keypoints_with_scores\n",
    "is_dynamic_shape_model = input_details[0]['shape_signature'][2] == -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9ccd016-282c-4497-91df-09ac53e0dc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_dynamic_shape_model = input_details[0]['shape_signature'][2] == -1\n",
    "if is_dynamic_shape_model:\n",
    "    input_tensor_index = input_details[0]['index']\n",
    "    input_shape = frame.shape\n",
    "    interpreter.resize_tensor_input(\n",
    "        input_tensor_index, input_shape, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649cda9a-2fc0-4068-9faa-023c80617a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e490d29f-5c49-4e6d-bc3f-15c0bdf632de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 17, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(keypoints_with_scores,[480,640,1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d901697a-e47d-47db-a9b6-8b843b4ce828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1, 1)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "# array = np.zeros((3,1,1))\n",
    "# print(array.shape)\n",
    "# array = np.squeeze(array)\n",
    "# print(array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a76c08-174a-4345-b2f7-304bd184f26d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45b386e5-c11d-47d4-926b-c58c73e345b0",
   "metadata": {},
   "source": [
    "# Draw Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09b6c2d7-1c43-4132-b49e-d85527f44e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoints(frame,keypoints,confidence):\n",
    "    y,x,c = frame.shape\n",
    "    shaped_keypoints = np.squeeze(np.multiply(keypoints,[y,x,1]))\n",
    "    for person_keypoint in shaped_keypoints:\n",
    "        for kp in person_keypoint:\n",
    "            ky,kx,kc = kp\n",
    "            if kc > confidence:\n",
    "                cv.circle(frame,(int(kx),int(ky)),4,(0,255,0),-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aa35d5-9e64-40ac-a7e7-9461081c8c9b",
   "metadata": {},
   "source": [
    "# Draw Edges or connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4030a21b-8f93-4f80-a899-02aa18c30170",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGES = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "34d6297c-4837-4313-b66c-97157ee29bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "def draw_keypoints_connections(frame,keypoints,edges,confidence_threshold):\n",
    "    y,x,c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints,[y,x,1]))\n",
    "    \n",
    "    for points, color in EDGES.items():\n",
    "        p1,p2 = points\n",
    "        for person in shaped:\n",
    "            y1,x1,c1 = person[p1]\n",
    "            y2,x2,c2 = person[p2]\n",
    "            if (c1 > confidence_threshold) & (c2 > confidence_threshold):\n",
    "                cv.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (math.floor(random.random() * 100),math.floor(random.random() * 160),math.floor(random.random() * 255)), 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
